{"componentChunkName":"component---src-templates-mdx-post-js","path":"/posts/lru-cache","webpackCompilationHash":"058093e3f59662845937","result":{"data":{"mdx":{"frontmatter":{"toc":false,"title":"Implementing an efficient LRU cache for JavaScript","subtitle":"Where we discover how to harness the power of JavaScript's typed array to design our very own low-cost pointer system for fixed-capacity data structures"},"tableOfContents":{"items":[{"url":"#the-lru-cache","title":"The LRU cache"},{"url":"#implementing-a-lru-cache","title":"Implementing a LRU cache"},{"url":"#doubly-linked-lists","title":"Doubly-linked lists"},{"url":"#lru-cache-list-operations","title":"LRU Cache list operations"},{"url":"#implementing-doubly-linked-lists-in-javascript","title":"Implementing doubly-linked lists in JavaScript"},{"url":"#a-custom-pointer-system","title":"A custom pointer system"},{"url":"#but-is-it-really-worth-the-hassle","title":"But is it really worth the hassle?"},{"url":"#parting-words","title":"Parting words"},{"url":"#miscellany","title":"Miscellany","items":[{"url":"#about-evictions--splay-trees","title":"About evictions & splay trees"},{"url":"#what-about-asmjs--webassembly","title":"What about asm.js & webassembly?"},{"url":"#saving-one-pointer-array","title":"Saving one pointer array"},{"url":"#about-arbitrary-evictions","title":"About arbitrary evictions"},{"url":"#a-fully-dynamic-custom-pointer-system","title":"A fully dynamic custom pointer system"}]},{"url":"#links","title":"Links"}]},"code":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"slug\": \"lru-cache\",\n  \"date\": \"2019-06-24\",\n  \"toc\": false,\n  \"title\": \"Implementing an efficient LRU cache for JavaScript\",\n  \"subtitle\": \"Where we discover how to harness the power of JavaScript's typed array to design our very own low-cost pointer system for fixed-capacity data structures\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"small\", null, mdx(\"a\", {\n    href: \"#implementing-doubly-linked-lists-in-javascript\"\n  }, \"I already know about LRU caches and doubly-linked lists. Bring me to the juicy part dammit!\")), mdx(\"p\", null, \"Let's say we need to process a very large, hundreds of gigabytes large, csv file containing urls we will need to fetch. To ensure we are not going to run out of memory while parsing the whole file, we need to read the file line by line:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"csv.forEachLine(line => {\\n  fetch(line.url);\\n});\\n\")), mdx(\"p\", null, \"Now let's say the person that created this file forgot to deduplicate the urls.\", mdx(SideNote, {\n    id: \"urls-dedup\",\n    mdxType: \"SideNote\"\n  }, \"I am aware that we could easily solve the issue by using \", mdx(\"code\", null, \"sort -u\"), \" but for the purpose of the demonstration, let's imagine this kind of solution does not exist.\"), mdx(SideNote, {\n    id: \"ural\",\n    mdxType: \"SideNote\"\n  }, \"Deduping urls is not as straigthforward as it may seem. Check the \", mdx(SafeLink, {\n    href: \"https://github.com/medialab/ural#readme\",\n    mdxType: \"SafeLink\"\n  }, \"ural\"), \" library in python, for instance, for some examples of what can be achieved in this regard.\"), \"This is an issue because we don't want to fetch the same url more than once: grabbing resources from the web is time-consuming & should be done parcimoniously not to overflow the sites you are grabbing those resources from.\"), mdx(\"p\", null, \"An easy solution could be to remember the url we already fetched by storing them into a map. Our pseudo-code would be changed to the following:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"const done = new Map();\\n\\ncsv.forEachLine(line => {\\n  if (done.has(line.url))\\n    return done.get(line.url);\\n\\n  const result = fetch(line.url);\\n  done.set(line.url, result);\\n});\\n\")), mdx(\"p\", null, \"At this point, the astute reader has easily noticed that we just defeated the purpose of reading the csv file line by line since we now need to commit all its urls to memory.\"), mdx(\"p\", null, \"And herein lies the issue: we need a way not to fetch the same urls too much while also making sure we won't run out of memory.\"), mdx(\"p\", null, mdx(MarginNote, {\n    mdxType: \"MarginNote\"\n  }, mdx(\"em\", null, \"Fun fact\"), \": if you work with data coming from the Internet such as lists of crawled urls, you will inevitably stumble upon this kind of power law. It naturally occurs because people link exponentially more to \", mdx(\"code\", null, \"twitter.com\"), \" than \", mdx(\"code\", null, \"unknownwebsite.fr\"), \".\"), \"Fortunately for us, it seems that only a tiny fraction of the urls contained in our file are repeated very often while the vast majority of others only appears one or two times. We can leverage this fact by designing a policy to throw away urls from our map if we have a good intuition they are unlikely to appear again. Thus, we won't allow our map to exceed a predetermined amount of memory.\"), mdx(\"p\", null, \"As such, one of the most commonly used eviction policy for such cases is called \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LRU\"), \", for \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"L\"), \"east \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"R\"), \"ecently \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"U\"), \"sed.\"), mdx(\"h1\", null, \"The LRU cache\"), mdx(\"p\", null, \"Hence, we understand that a LRU cache is a fixed-capacity map able to bind values to keys with the following twist: if the cache is full but we need to insert a new key-value pair, it will make some place by evicting the least recently used key-value pair.\"), mdx(\"p\", null, \"To do so, the cache will need to store given pairs in order of their last access. This means that each time someone tries to set a new key, or access a key, we need to modify the underlying list of pairs to ensure the needed order is maintained.\"), mdx(\"p\", null, mdx(MarginNote, {\n    mdxType: \"MarginNote\"\n  }, mdx(\"em\", null, \"Note\"), \": LFU (Least Frequently Used) is a perfectly valid cache eviction policy. It's just less widespread. You can read about it \", mdx(SafeLink, {\n    href: \"https://en.wikipedia.org/wiki/Cache_replacement_policies#Least-frequently_used_(LFU)\",\n    mdxType: \"SafeLink\"\n  }, \"here\"), \" and find implementation notes \", mdx(SafeLink, {\n    href: \"https://www.geeksforgeeks.org/lfu-least-frequently-used-cache-implementation/\",\n    mdxType: \"SafeLink\"\n  }, \"here\")), \"But why is this order relevant? Why not record the number of times each pair is accessed instead and evict the least frequently used item? Here are some reasons why:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"LRU is a actually a good proxy of LFU since the more a pair is accessed, the less chance it has to be evicted.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"You will need to store integers in addition to everything else to keep track of the number of times the pairs were accessed.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Infering the order of pair access is very straightforward since it can be deduced from the order of operations on the cache.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"With LFU, you will often need to make an arbitrary choice of which pair to evict, for instance if all your pairs have been accessed only once. With LRU, you don't have such choice to make: you just evict the least recently used, which cannot be ambiguous.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"LRU is quite straightforward to implement, LFU is not.\")), mdx(\"h1\", null, \"Implementing a LRU cache\"), mdx(\"p\", null, \"There are many ways to implement a working LRU cache but I will only focus here on the way you are most likely to encounter in the wild when developing with high-level languages.\"), mdx(\"p\", null, \"Usually, to implement a proper LRU cache, one will need the two following ingredients:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"A hashmap-like data structure able to retrieve values associated to arbitrary keys - such as strings - efficiently. In JavaScript we can use either the ES6 \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"Map\"), \" or any plain object \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"{}\"), \": remember that our cache is no different from a fixed-capacity key-value store.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"A way to store our pairs in the order of their last access. What's more, we will need to be able to travers the list in both normal and reversed order. That's why people naturally lean toward a \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://en.wikipedia.org/wiki/Doubly_linked_list\"\n  }), \"doubly-linked list\"), \" to do the job.\")), mdx(\"p\", null, \"Minimally, your implementation needs to be able to run the two following operations:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"#.set\"), \": associating a value to the given key, while evicting the least recently used item if the cache is full.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"#.get\"), \": retrieving the value associated to the given key if this one exists at all in the cache.\")), mdx(\"p\", null, \"And here is how we could use such a cache:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"// Let's create a cache able to contain 3 items\\nconst cache = new LRUCache(3);\\n\\n// Let's add items\\ncache.set(1, 'one');\\ncache.set(2, 'two');\\ncache.set(3, 'three');\\n\\n// Up until now, nothing was evicted from the cache\\ncache.get(2);\\n>>> 'two'\\n\\n// Oh no! we need to add a new pair\\ncache.set(4, 'four');\\n\\n// `1` was evicted because it was the least recently used key\\ncache.get(1);\\n>>> undefined\\n\\n// If you get `2`, it won't be the LRU anymore\\ncache.get(2);\\n>>> 'two'\\n\\n// Which means that it's `3` that will be evicted now!\\ncache.set(5, 'five');\\ncache.get(3);\\n>>> undefined\\n\\n// Thus we never store more than 3 pairs\\ncache.size\\n>>> 3\\n\")), mdx(\"h1\", null, \"Doubly-linked lists\"), mdx(\"div\", {\n    className: \"paragraph\"\n  }, mdx(MarginNote, {\n    mdxType: \"MarginNote\"\n  }, mdx(\"em\", null, \"Question\"), \": Why isn't a singly-linked list enough in our case? Because we have to efficiently perform the following operations on our list:\", mdx(\"ol\", null, mdx(\"li\", null, \"place an item at the beginning of the list\"), mdx(\"li\", null, \"move an item from anywhere in the list to its beginning\"), mdx(\"li\", null, \"remove the last item from the list while keeping a correct pointer to the new last item\"))), \"To be able to implement our LRU cache, we will first need to implement a doubly-linked list to make sure we are able to store our items in order of their last access: the least recently used item will be at the end of this list while the most recently used will be at its beginning.\"), mdx(\"p\", null, \"So how do we represent a doubly-linked list in memory? Usually, we do so by creating a structure containing the following three elements:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"A payload, i.e. the actual list item. It can be anything, really, from an string to an integer etc.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"A pointer towards the previous element in the list.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"A pointer towards the next element in the list.\")), mdx(\"p\", null, \"Then we also need to store a pointer to both the first & the last element of the list and we are done.\"), mdx(\"p\", null, \"Schematically, it would look somewhat like this:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-c\"\n  }), \"a node:\\n\\n  (prev|payload|next)\\n\\n  payload: the stored value. here, it's a string\\n  prev: pointer to previous item\\n  next: pointer to next item\\n  \\u2022: pointer\\n  x: null pointer\\n\\na list:\\n\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"one\\\"|\\u2022)  (\\u2022|\\\"two\\\"|\\u2022)  (\\u2022|\\\"three\\\"|x) \\u2022 tail\\n              \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518    \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\")), mdx(\"h1\", null, \"LRU Cache list operations\"), mdx(\"p\", null, \"As long as the cache is not full, it is quite easy to maintain our list of cached items. We just need to prepend the newly inserted items into the list:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-c\"\n  }), \"1. an empty cache with capacity 3\\n\\n\\n  head x     x tail\\n\\n\\n2. let's cache the key \\\"one\\\"\\n\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"one\\\"|x) \\u2022 tail\\n              \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\\n3. let's cache the key \\\"two\\\" (notice \\\"two\\\" comes at the front)\\n\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"two\\\"|\\u2022)  (\\u2022|\\\"one\\\"|x) \\u2022 tail\\n              \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\\n4. finally we cache the key \\\"three\\\"\\n\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510    \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"three\\\"|\\u2022)  (\\u2022|\\\"two\\\"|\\u2022)  (\\u2022|\\\"one\\\"|x) \\u2022 tail\\n               \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\")), mdx(\"p\", null, \"So far so good. Now, to keep our list in LRU order, if anyone accesses an already stored key in the cache we will need to reorder the list by moving said key to the front:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-c\"\n  }), \"1. the current state of our cache\\n\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510    \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"three\\\"|\\u2022)  (\\u2022|\\\"two\\\"|\\u2022)  (\\u2022|\\\"one\\\"|x) \\u2022 tail\\n               \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\\n2. we access the key \\\"two\\\", we first extract it from the list and\\n   rewire previous & next items.\\n\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510    \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"three\\\"|\\u2022)  (\\u2022|\\\"one\\\"|x) \\u2022 tail\\n               \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\\n  dangling: (x|\\\"two\\\"|x)\\n\\n3. then we move the node to the front\\n\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510    \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"two\\\"|\\u2022)  (\\u2022|\\\"three\\\"|\\u2022)  (\\u2022|\\\"one\\\"|x) \\u2022 tail\\n              \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518    \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\")), mdx(\"p\", null, \"Note that each time, we need to update the head pointer and that, sometimes, we also need to update the tail pointer.\"), mdx(\"p\", null, \"Finally, if the cache is already full and we need to insert a yet unknown key, we will need to pop the last item from the list to make place so we can prepend the new one.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-c\"\n  }), \"1. the current state of our cache\\n\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510    \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"two\\\"|\\u2022)  (\\u2022|\\\"three\\\"|\\u2022)  (\\u2022|\\\"one\\\"|x) \\u2022 tail\\n              \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518    \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\\n2. we need to insert the key \\\"six\\\" but the cache is full\\n   we first need to pop \\\"one\\\", being the LRU item\\n\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"two\\\"|\\u2022)  (\\u2022|\\\"three\\\"|\\u2022) \\u2022 tail\\n              \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518    \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\\n3. then we prepend our new item\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"six\\\"|\\u2022)  (\\u2022|\\\"two\\\"|\\u2022)  (\\u2022|\\\"three\\\"|x) \\u2022 tail\\n              \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518    \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\")), mdx(\"p\", null, \"Here is all you need to know about doubly-linked lists to be able to implement a decent LRU cache.\"), mdx(\"h1\", null, \"Implementing doubly-linked lists in JavaScript\"), mdx(\"p\", null, \"Now we have a slight issue: the JavaScript language does not have pointers per se. Indeed we can only work by passing references around, represented as object properties.\"), mdx(\"p\", null, \"This means that, usually, most people will implement linked lists in JavaScript by relying on classes that would look like the following:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"// One class for the nodes\\nfunction DoublyLinkedListNode(value) {\\n  this.value = value;\\n  this.previous = null;\\n  this.next = null;\\n}\\n\\n// One class for the list\\nfunction DoublyLinkedList() {\\n  this.head = null;\\n  this.tail = null;\\n}\\n\\n// Performing operations (should be wrapped in the list's method)\\nconst list = new DoublyLinkedList();\\nconst node1 = new DoublyLinkedList('one');\\nconst node2 = new DoublyLinkedList('two');\\n\\nlist.head = node1;\\nlist.tail = node2;\\nnode1.next = node2;\\nnode2.previous = node1;\\n// ...\\n\")), mdx(\"p\", null, \"While there is nothing wrong with this approach, it still has its drawback that will make any similar-looking implementation quite bad, performance-wise:\", mdx(SideNote, {\n    id: \"linked-list-bad-perf\",\n    mdxType: \"SideNote\"\n  }, \"This is why you won't see many people using linked lists in application JavaScript code.\", mdx(\"br\", null), mdx(\"br\", null), \"Only people needing it for very specific use cases where they algorithmically shine will actually use them.\", mdx(\"br\", null), mdx(\"br\", null), \"node.js has such an implementation \", mdx(SafeLink, {\n    href: \"https://github.com/nodejs/node/blob/master/lib/internal/linkedlist.js\",\n    mdxType: \"SafeLink\"\n  }, \"here\"), \" and you can find it used for timers \", mdx(SafeLink, {\n    href: \"https://github.com/nodejs/node/blob/master/lib/internal/timers.js\",\n    mdxType: \"SafeLink\"\n  }, \"here\"))), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Each time you instantiate a node, some superflous memory will be allocated for the object's bookkeeping.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"If your list moves fast, i.e. nodes are often added or removed, it will trigger garbage collection, whose behavior is, in JavaScript, beyond your control.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Engines will try to optimize your objects as low-level structs most of the time, as opposed to a hashmap, but you have no control over it.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Finally, and this is related to \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"3.\"), \", object property access is not the fastest thing in the world.\")), mdx(\"p\", null, \"But here we can be a little more clever than that. Indeed, there is a characteristic of our linked list that we can leverage to be more performant: its capacity is fixed.\"), mdx(\"p\", null, \"So, instead of using JavaScript references & properties as pointers, let's roll our own pointer system using \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray\"\n  }), \"Typed Arrays\"), \"!\"), mdx(\"h1\", null, \"A custom pointer system\"), mdx(\"p\", null, \"Typed Arrays are very handy JavaScript objects able to represent fixed-capacity arrays containing a certain amount of typical number types such as \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"int32\"), \" or \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"float64\"), \" and so on...\"), mdx(\"p\", null, \"They are fairly fast and consume very little memory because the stored items are statically typed and therefore induce no bookkeeping overhead.\"), mdx(\"p\", null, \"Here is how you would use one:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"// Let's create a typed array containing 256 unsigned 16 bits integers\\nconst array = new Uint16Array(256);\\n\\n// Every item is initially set to 0.\\n// You can read/set items the same way you would with a vanilla Array\\narray[10] = 34;\\narray[10];\\n>>> 34\\n\\n// But here's the catch: you cannot push new items\\narray.push(45);\\n>>> throw `TypeError: array.push is not a function`\\n\")), mdx(\"p\", null, mdx(MarginNote, {\n    mdxType: \"MarginNote\"\n  }, \"What's more, instantiating a typed array in JavaScript is not so far, conceptually, from calling a \", mdx(\"code\", null, \"malloc\"), \" in C. Or, at least, you can somewhat use them to perform the same kind of tasks in both languages.\"), \"Since we can now use those very performant arrays, why shouldn't we use them to implement our own pointer system? After all, pointers are nothing more than addresses mapping a chunk of memory.\"), mdx(\"p\", null, \"Then let's use a typed array as our chunk of memory and let's use indices into it as our addresses! The only tricky part is to correctly choose the integer type, relative to our capacity, to avoid overflows.\", mdx(SideNote, {\n    id: \"get-pointer-array\",\n    mdxType: \"SideNote\"\n  }, \"If you are ever unsure how to develop this, check this function right \", mdx(SafeLink, {\n    href: \"https://github.com/Yomguithereal/mnemonist/blob/7ea90e6fec46b4c2283ae88f173bfb19ead68734/utils/typed-arrays.js#L8-L54\",\n    mdxType: \"SafeLink\"\n  }, \"here\"))), mdx(\"p\", null, \"So, in order to implement a fixed-capacity doubly-linked list in JavaScript to serve our LRU cache structure, we'll need the following typed arrays:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"A vanilla array to store our list's payloads, i.e. key-value pairs. Or two typed or vanilla arrays to store keys & values separately and depending on their respective types. For instance, if we have the guarantee our value cannot be anything else than 32 bits integers, we can again leverage typed arrays for that.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"A typed array storing a bunch of indices representing our next pointers.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Another one to store the indices representing our previous pointers.\")), mdx(\"div\", {\n    className: \"paragraph\"\n  }, mdx(MarginNote, {\n    mdxType: \"MarginNote\"\n  }, \"Oftentimes, when using the \", mdx(\"em\", null, \"typed array storing pointers\"), \" trick is a bit tedious to code when you need to ensure the \", mdx(\"code\", null, \"0\"), \" index represents a null value or pointer.\", mdx(\"br\", null), mdx(\"br\", null), \"Two somewhat crafty ways to circumvent this issue:\", mdx(\"ol\", null, mdx(\"li\", null, \"You can offset your values array by keeping the first item empty so that the \", mdx(\"code\", null, \"0\"), \" index does not risk storing anything important.\"), mdx(\"li\", null, \"You can offset your indices by one but this often requires to perform some light arithmetic on the indices which make the code quite unpalatable and complicated to understand.\")), \"Note that people also use the trick of using signed typed arrays rather than unsigned ones (indices cannot be negative numbers, obviously) to add one level of indirection: a pointer can signify one thing or the other based on the sign of the index.\", mdx(\"br\", null), mdx(\"br\", null), \"You can use this trick, for instance, to flag nodes in a \", mdx(SafeLink, {\n    href: \"https://en.wikipedia.org/wiki/Trie\",\n    mdxType: \"SafeLink\"\n  }, \"Trie\"), \" as leaves, by using negative indices, to save up some memory.\"), \"Our code would subsequently look something like this:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"function FixedCapacityDoublyLinkedList(capacity) {\\n  this.keys = new Array(capacity);\\n  this.values = new Uint8Array(capacity);\\n\\n  this.next = new Uint8Array(capacity);\\n  this.previous = new Uint8Array(capacity);\\n\\n  this.head = 0;\\n  this.tail = 0;\\n}\\n\\n// And if we recall this example from before:\\nconst list = new DoublyLinkedList();\\nconst node1 = new DoublyLinkedList('one');\\nconst node2 = new DoublyLinkedList('two');\\n\\nlist.head = node1;\\nlist.tail = node2;\\nnode1.next = node2;\\nnode2.previous = node1;\\n\\n// This now would look more like this:\\nconst list = new FixedCapacityDoublyLinkedList(2);\\n\\n// First node\\nlist.keys[0] = 'one';\\nlist.values[0] = 1;\\n\\n// Second node\\nlist.keys[1] = 'two';\\nlist.values[1] = 2;\\n\\n// Wiring & pointer mangling\\nlist.next[0] = 1;\\nlist.previous[1] = 0;\\n\\nlist.head = 0;\\nlist.tail = 1;\\n\")), mdx(\"p\", null, \"Now, instead of setting node instances properties, we lookup & set indices in typed arrays.\"), mdx(\"p\", null, \"Schematically, again, here is what we are doing:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-c\"\n  }), \"To represent the following list:\\n\\n       \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510    \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510   \\u250C\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500>\\u2510\\n  head \\u2022 (x|\\\"three\\\"|\\u2022)  (\\u2022|\\\"two\\\"|\\u2022)  (\\u2022|\\\"one\\\"|x) \\u2022 tail\\n               \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518   \\u2514<\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\\n\\nWe will store the following indices & arrays:\\n\\n  head     = 2\\n  tail     = 0\\n  capacity = 3\\n  size     = 3\\n\\n  index       0      1       2\\n  values = [\\\"one\\\", \\\"two\\\", \\\"three\\\"]\\n  prev   = [    1,     2,       x]\\n  next   = [    x,     0,       1]\\n\\nx (null pointer) should be 0 here: it's easier\\nto reason about likewise, so you are not hampered\\nby squishy implementation details\\n\")), mdx(\"p\", null, \"So, if we \\u201Crerun\\u201D our \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"#lru-cache-related-doubly-linked-lists-operations\"\n  }), \"previous examples\"), \" about list operations needed by our LRU cache using our new scheme:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-c\"\n  }), \"1. We start with an empty list\\n\\n  head     = x\\n  tail     = x\\n  capacity = 3\\n  size     = 0\\n\\n  index       0      1      2\\n  values = [    x,     x,     x]\\n  prev   = [    x,     x,     x]\\n  next   = [    x,     x,     x]\\n\\n2. We insert \\\"one\\\"\\n\\n  head     = 0\\n  tail     = 0\\n  capacity = 3\\n  size     = 1\\n\\n  index       0      1      2\\n  values = [\\\"one\\\",     x,     x]\\n  prev   = [    x,     x,     x]\\n  next   = [    x,     x,     x]\\n\\n3. We insert \\\"two\\\"\\n\\n  head     = 1\\n  tail     = 0\\n  capacity = 3\\n  size     = 2\\n\\n  index       0      1      2\\n  values = [\\\"one\\\", \\\"two\\\",     x]\\n  prev   = [    1,     x,     x]\\n  next   = [    x,     0,     x]\\n\\n4. We insert \\\"three\\\"\\n\\n  head     = 2\\n  tail     = 0\\n  capacity = 3\\n  size     = 3\\n\\n  index       0      1       2\\n  values = [\\\"one\\\", \\\"two\\\", \\\"three\\\"]\\n  prev   = [    1,     2,       x]\\n  next   = [    x,     0,       1]\\n\\n5. We access \\\"two\\\" and bring it the the front of the list\\n\\n  head     = 1\\n  tail     = 0\\n  capacity = 3\\n  size     = 3\\n\\n  index       0      1       2\\n  values = [\\\"one\\\", \\\"two\\\", \\\"three\\\"]\\n  prev   = [    2,     x,       1]\\n  next   = [    x,     2,       0]\\n\\n6. Finally we insert \\\"six\\\" and evict \\\"one\\\"\\n\\n  head     = 0\\n  tail     = 2\\n  capacity = 3\\n  size     = 3\\n\\n  index       0      1       2\\n  values = [\\\"six\\\", \\\"two\\\", \\\"three\\\"]\\n  prev   = [    1,     0,       1]\\n  next   = [    x,     2,       x]\\n\")), mdx(\"p\", null, \"It looks boring, no?\"), mdx(\"p\", null, \"Well it's actually a good thing. Because it means that we don't move things around too much. We don't create things, we just read & write array indices.\"), mdx(\"p\", null, \"So why is this faster than the traditional implementation with object properties we saw earlier?\"), mdx(\"ol\", null, mdx(\"li\", null, \"We only allocate memory once. We never instantiate new objects. We never need to garbage collect old ones.\", mdx(SideNote, {\n    id: \"pool\",\n    mdxType: \"SideNote\"\n  }, \"Yes you can temper memory allocation and garbage collection by using object pools. But if you can rely on typed arrays instead you'll find that it's faster and consumes less memory.\"), \"And memory predictability is a desirable thing to have in a LRU cache implementation. The only memory hiccups in our implementation will be the result of setting keys in our map or object up to capacity then evicting keys later on.\"), mdx(\"li\", null, \"Array indices lookups/writes are really fast because the allocated memory is mostly contiguous, even more with typed arrays. You never need to jump very far to find what you need and cache optimizations can more easily leverage their magic.\"), mdx(\"li\", null, \"It leaves nothing to interpretation so that the engine does not have to be clever about anything and will be able to automatically take the fast paths relying on very low-level optimizations.\")), mdx(\"h1\", null, \"But is it really worth the hassle?\"), mdx(\"p\", null, \"To make sure of this, I obviously had to implement such a custom pointer system and benchmark it.\"), mdx(\"p\", null, \"You can therefore find a typed array-based implementation of a LRU cache in the \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/Yomguithereal/mnemonist\"\n  }), \"mnemonist\"), \" library. You will find one implementation relying on a JavaScript object: the \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://yomguithereal.github.io/mnemonist/lru-cache\"\n  }), mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"LRUCache\")), \", and another one relying on an ES6 \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Map\"), \": the \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://yomguithereal.github.io/mnemonist/lru-map\"\n  }), mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"LRUMap\")), \".\"), mdx(\"p\", null, \"Then there is this public benchmark on the \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/dominictarr/bench-lru\"\n  }), \"dominictarr/bench-lru\"), \" repository.\"), mdx(\"p\", null, \"And here are some of the latest results:\", mdx(SideNote, {\n    id: \"bench\",\n    mdxType: \"SideNote\"\n  }, \"Note that I hid results from the \", mdx(SafeLink, {\n    href: \"https://npmjs.com/package/hashlru\",\n    mdxType: \"SafeLink\"\n  }, \"hashlru\"), \" and \", mdx(SafeLink, {\n    href: \"https://npmjs.com/package/quick-lru\",\n    mdxType: \"SafeLink\"\n  }, \"quick-lru\"), \" because they are not traditional LRU caches and consume twice the required memory.\", mdx(\"br\", null), mdx(\"br\", null), \"They still have, mostly the first one, good write performance but somewhat less good read performance.\")), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"library\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"set\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"get1\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"update\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"get2\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"evict\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), mdx(\"a\", _extends({\n    parentName: \"td\"\n  }, {\n    \"href\": \"https://www.npmjs.com/package/mnemonist\"\n  }), \"mnemonist-object\")), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"15314\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"69444\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"35026\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"68966\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"7949\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), mdx(\"a\", _extends({\n    parentName: \"td\"\n  }, {\n    \"href\": \"https://npmjs.com/package/tiny-lru\"\n  }), \"tiny-lru\")), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"6530\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"46296\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"37244\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"42017\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"5961\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), mdx(\"a\", _extends({\n    parentName: \"td\"\n  }, {\n    \"href\": \"https://npmjs.com/package/lru-fast\"\n  }), \"lru-fast\")), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"5979\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"36832\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"32626\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"40900\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"5929\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), mdx(\"a\", _extends({\n    parentName: \"td\"\n  }, {\n    \"href\": \"https://www.npmjs.com/package/mnemonist\"\n  }), \"mnemonist-map\")), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"6272\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"15785\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"10923\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"16077\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"3738\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), mdx(\"a\", _extends({\n    parentName: \"td\"\n  }, {\n    \"href\": \"https://www.npmjs.com/package/lru\"\n  }), \"lru\")), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"3927\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"5454\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"5001\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"5366\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"2827\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), mdx(\"a\", _extends({\n    parentName: \"td\"\n  }, {\n    \"href\": \"https://npmjs.com/package/simple-lru-cache\"\n  }), \"simple-lru-cache\")), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"3393\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"3855\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"3701\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"3899\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"2496\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), mdx(\"a\", _extends({\n    parentName: \"td\"\n  }, {\n    \"href\": \"https://npmjs.com/package/hyperlru-object\"\n  }), \"hyperlru-object\")), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"3515\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"3953\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"4044\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"4102\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"2495\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), mdx(\"a\", _extends({\n    parentName: \"td\"\n  }, {\n    \"href\": \"https://www.npmjs.com/package/js-lru\"\n  }), \"js-lru\")), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"3813\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"10010\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"9246\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"10309\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"1843\")))), mdx(\"p\", null, \"mnemonist's custom pointer system fares quite well.\"), mdx(\"p\", null, \"What's more, you'll see by reading its \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/Yomguithereal/mnemonist/blob/master/lru-cache.js\"\n  }), \"source code\"), \" that it's not too horrible to read and remains quite simple.\"), mdx(\"h1\", null, \"Parting words\"), mdx(\"p\", null, \"Now you know how to use JavaScript typed arrays to create your own pointer system. This trick is not limited to fixed-capacity linked lists and can be used for a variety or other data structure implementation problems.\", mdx(SideNote, {\n    id: \"trees\",\n    mdxType: \"SideNote\"\n  }, \"A lot of tree-like data structures may also beneficiate from this trick, for instance.\")), mdx(\"p\", null, \"But as per usual, and this advice stands for most high-level languages\", mdx(SideNote, {\n    id: \"python\",\n    mdxType: \"SideNote\"\n  }, \"The \", mdx(\"em\", null, \"typed array storing pointers\"), \" trick is far from suited to every high-level language. In python, for instance, if you try to replicate this trick using \", mdx(\"code\", null, \"bytearray\"), \" or \", mdx(\"code\", null, \"np.array\"), \" you will actually get abysmal performances.\"), \", optimizing JavaScript is the same as squinting really hard and pretending the language:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"has static typing\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"is low-level\")), mdx(\"p\", null, \"Basically, the less choices the engine has to make, the easier it will be for it to optimize your code.\"), mdx(\"p\", null, \"Of course this is not advisable for application code and this level of optimization should only be a goal if you try to optimize critical things such as, off the top of my head, a LRU cache.\", mdx(SideNote, {\n    id: \"memoize\",\n    mdxType: \"SideNote\"\n  }, \"LRU cache are very important to many implementations of memoization, for instance.\", mdx(\"br\", null), mdx(\"br\", null), \"Web servers and clients alike also massively rely on those kinds of caches.\")), mdx(\"p\", null, \"And finally, please don't take my words or advices too seriously. It is notoriously tricky to optimize interpreted languages and JavaScript is even worse because you have to consider JIT compilation and several engines such as Gecko or V8.\"), mdx(\"p\", null, \"So, pretty please, do benchmark your code because your mileage may vary.\"), mdx(\"p\", {\n    align: \"center\"\n  }, mdx(\"br\", null), \"Have a good day!\"), mdx(Divider, {\n    mdxType: \"Divider\"\n  }), mdx(\"h1\", null, \"Miscellany\"), mdx(\"h2\", null, \"About evictions & splay trees\"), mdx(\"p\", null, \"The single thing hampering every JavaScript implementation of a LRU cache is actually evictions. Getting rid of keys from either an object (using the \", mdx(\"code\", null, \"delete\"), \" keyword) or a map (using the \", mdx(\"code\", null, \"#.delete\"), \" method) is very costly.\", mdx(SideNote, {\n    id: \"hashlru\",\n    mdxType: \"SideNote\"\n  }, \"Once again the \", mdx(SafeLink, {\n    href: \"https://npmjs.com/package/hashlru\",\n    mdxType: \"SafeLink\"\n  }, \"hashlru\"), \" and \", mdx(SafeLink, {\n    href: \"https://npmjs.com/package/quick-lru\",\n    mdxType: \"SafeLink\"\n  }, \"quick-lru\"), \" libraries formulate an original solution to this issue and I warmly encourage you to check them.\")), mdx(\"p\", null, \"There seems to be no way around it because it is quite impossible (yet?) to beat the engines native hashmaps' performance from within interpreted JavaScript code.\", mdx(SideNote, {\n    id: \"hashmaps\",\n    mdxType: \"SideNote\"\n  }, \"The contrary would be very counterintuitive since there is no way to run hash algorithms as fast the engine is able to do natively.\", mdx(\"br\", null), mdx(\"br\", null), \"I also attempted other tree-based key-value associative data structures like \", mdx(SafeLink, {\n    href: \"https://cr.yp.to/critbit.html\",\n    mdxType: \"SafeLink\"\n  }, \"CritBit trees\"), \" but must report that it's not possible to beat a JavaScript object or map thusly.\", mdx(\"br\", null), mdx(\"br\", null), \"You can still implement those trees so that they can be from 2 to 5 times less performant only than native objects for certain use case, all while maintaining lexicographical orde. Which is not too shabby. I guess?\", mdx(\"br\", null), mdx(\"br\", null), \"Feel free to check the undocumented code \", mdx(SafeLink, {\n    href: \"https://github.com/Yomguithereal/mnemonist/blob/master/critbit-tree-map.js\",\n    mdxType: \"SafeLink\"\n  }, \"here\"), \".\")), mdx(\"p\", null, \"This means that you can't roll your own map in JavaScript, a fixed-capacity one with inexpensive deletion for instance, and hope to beat what's already provided to you with the native object.\"), mdx(\"p\", null, \"One interesting solution that would be nice to test is using \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://en.wikipedia.org/wiki/Splay_tree\"\n  }), \"splay trees\"), \".\"), mdx(\"p\", null, \"Those trees, being a binary search tree variant, support rather efficient associative key-value operations while being very suited to LRU caches since they already work by \\u201Csplaying\\u201D very frequently accessed key to the top of their hierarchy.\"), mdx(\"h2\", null, \"What about asm.js & webassembly?\"), mdx(\"p\", null, \"Wouldn't it be faster to implement a LRU cache using shiny new things such a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"asm.js\"), \" or \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"webassembly\"), \" rather than trying to shoehorn low-level concepts into JavaScript high-level code?\"), mdx(\"p\", null, \"Well yes. But here is the trick: if you ever need to use this implementation from the JavaScript side, then you are out of luck because it will be slow as hell. Just because communication between the JavaScript side and the, say, webassembly side will slow you down, just a little, but enough to make such an implementation moot.\", mdx(SideNote, {\n    id: \"wasm-communication\",\n    mdxType: \"SideNote\"\n  }, \"Communication between wasm and JS has been wildly improving. Check this very good \", mdx(SafeLink, {\n    href: \"https://hacks.mozilla.org/2018/10/calls-between-javascript-and-webassembly-are-finally-fast-%F0%9F%8E%89/\",\n    mdxType: \"SafeLink\"\n  }, \"blog post\"), \" on the matter, for instance.\", mdx(\"br\", null), mdx(\"br\", null), \"But it's still not enough to justify having hot data structure methods run on the wasm side while being called from the JS side, unfortunately.\")), mdx(\"p\", null, \"However, if you can afford to stay in the webassembly side, because you compile rust to webassembly for instance, then it's a tremendous idea to also implement your LRU cache there. You will definitely get better results.\"), mdx(\"h2\", null, \"Saving one pointer array\"), mdx(\"p\", null, \"There is a known trick with LRU cache that enables you to save up one pointer level. It does not mean that you don't need a doubly-linked list anymore to be efficient but just that the hashmap/dictionary structure you are using can store pointers to the previous key-value pair rather than the related one to save up memory.\"), mdx(\"p\", null, \"I won't explain this here but you can head to this \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://stackoverflow.com/questions/49621983/lru-cache-with-a-singly-linked-list\"\n  }), \"stackoverflow thread\"), \" if you want to get the gist of it.\"), mdx(\"p\", null, \"Just note that it's usually not a good idea if you want to be remain performant (computation-wise, not memory-wise, obviously) because it will incur more hashmap lookups, and those are quite costly.\"), mdx(\"h2\", null, \"About arbitrary evictions\"), mdx(\"p\", null, \"Note that the LRU cache implementation proposed here will have a hard time handling arbitrary evictions, i.e. letting the user delete keys.\"), mdx(\"p\", null, \"Why? Because here, since we only need to swap key-value pairs when inserting a new key that will evict the LRU one, we don't need to have a way to find \\u201Cavailable\\u201D slots in our memory. Those slots, if one can delete keys at will, would then have random places in the typed array and we would need a way to find them back to \\u201Creallocate\\u201D them.\"), mdx(\"p\", null, \"This could be done by using an additional array serving as a free pointer stack, but this has obviously a memory and performance cost.\"), mdx(\"h2\", null, \"A fully dynamic custom pointer system\"), mdx(\"p\", null, \"Here I mostly spoke of typed array to implement fixed-capacity pointer systems. But if you are a little bit more ambitious you can very well imagine designing a dynamic-capacity pointer system.\"), mdx(\"p\", null, \"To do so, you can use dynamic typed arrays like mnemonist's \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://yomguithereal.github.io/mnemonist/vector.html\"\n  }), \"Vector\"), \". They can actually be more efficient than vanilla JavaScript arrays when handling numbers and will let you implement what you need.\"), mdx(\"p\", null, \"I am unsure whether having a custom dynamic pointer system would yield any performance improvement when implementing some other data structure however.\"), mdx(\"h1\", null, \"Links\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"You might find the \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://yomguithereal.github.io/mnemonist/\"\n  }), \"mnemonist\"), \" library useful. It contains a lot of efficient and cohesive data structure implementations for JavaScript.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"I did a \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://fosdem.org/2019/schedule/event/data_structures_javascript/\"\n  }), \"talk\"), \" about implementing data structures in JavaScript at FOSDEM in 2019.\")));\n}\nMDXContent.isMDXComponent = true;"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"lru-cache"}}}