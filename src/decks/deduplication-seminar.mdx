---
type: deck
draft: false
title: "Dupond & Dupont: La déduplication assistée par ordinateur"
slug: deduplication-seminar
date: "2019-10-01"
lang: fr
event: "Séminaire du médialab"
description:
    A guided tour of deduplication algorithms and their usage in a variety of
    social sciences projects.
---

import Head from 'gatsby-theme-mdx-deck/src/components/head';
import Notes from 'gatsby-theme-mdx-deck/src/components/notes';
import customTheme from '../style/decks-theme.js';
import {Verbatim} from './components/deduplication.js';

import DATA from './data/deduplication.json';

export const theme = customTheme;

<Head>
  <title>Dupond &amp; Dupont - la Déduplication Assistée par Ordinateur</title>
</Head>

<h2>Dupond &amp; Dupont:<br />la déduplication assistée par ordinateur</h2>

### Une visite guidée des algorithmes de déduplication et de leur utilisation dans le cadre de projets en sciences sociales

*Guillaume Plique, Séminaire du médialab de Sciences Po, 01/10/2019*

---

Lien: [https://yomguithereal.github.io/decks/deduplication-seminar](https://yomguithereal.github.io/decks/deduplication-seminar)

---

#### Présentation & règles du jeu

Qui a déja utilisé l'outil [Open Refine](http://openrefine.org/) (anciennement Google Refine) ?

<Notes>
  Présentation perso, interruptions, questions et si je pars dans des délires mystiques. Hobby et péché mignon. Fascination pour OpenRefine.
</Notes>

---

#### Les ordinateurs sont stupides

Ces deux chaînes de caractères sont elles similaires ?

<ul>
  <li>
    <Verbatim>Dupond</Verbatim>
  </li>
  <li>
    <Verbatim>Dupont</Verbatim>
  </li>
</ul>

---

#### Les ordinateurs n'ont aucun humour et ne sont pas tolérants à l'erreur

Ces deux chaînes de caractères sont elles similaires ?

<ul>
  <li>
    <code style={{backgroundColor: 'white', padding: '7px'}}>Dupond</code>
  </li>
  <li>
    <code style={{backgroundColor: 'white', padding: '7px'}}>Dupond&nbsp;</code>
  </li>
</ul>

---

#### Les ordinateurs se moquent de la réalité des sciences sociales

* Données d'archives (souvent historiques et pré-orthographe)
* Retranscriptions d'entretiens
* Données d'enquête remplies à plusieurs mains
* Fusions de multiples bases de données

---

#### Comment calculer des agrégations légitimes ?

Le nettoyage à base d'huile de coude.

Le cas du projet [TOFLIT18](http://toflit18.medialab.sciences-po.fr) et de l'histoire de l'orthographe:

<ul style={{overflowY: 'scroll', height: '300px', border: '1px dashed black'}}>
  {DATA.merrains.map(item => <li key={item}><code>{item}</code></li>)}
</ul>

---

#### Le travail manuel à l'heure du "Big Data"

TOFLIT18 c'est `~50k` libellés.

Le nettoyage *a la main* est-il encore possible si on multiplie ce chiffre par 10, 100, 1000 ?

---

#### La déduplication assistée par ordinateur

Démo [Takoyaki](https://yomguithereal.github.io/takoyaki/)

---

#### Un fantasme d'ingénieur (ou de dictature)

Monsieur `identifiant unique n°46251574`

Madame `identifiant unique n°7472575`

*« Ce n'est pas moi qui ai tort, c'est la réalité qui se trompe »*

---

#### Un début de solution

Emuler cet univers alternatif.

Plier les maths à la réalité: la [logique floue](https://en.wikipedia.org/wiki/Fuzzy_logic) de Lofti Zadeh (1965).

---

#### Un problème polymorphe mais pourtant unique

1. `fuzzy matching`: trouver les élements similaires à une requête `q` dans un ensemble `A`.
2. `similarity join`: trouver les similarités de tous les élements d'un ensemble `A` avec ceux d'un ensemble `B`.
3. `fuzzy clustering`: trouver, dans un ensemble `A` toutes les paires d'éléments similaires.

Tous ces problèmes sont mathématiquement **identiques**.

<Notes>
  Appuyer la similarité entre 2. et 3. et bien expliquer le problème 3. qui est celui dont on a fait la démonstration dans Takoyaki.
</Notes>

---

#### De multiples noms, dans des disciplines variées

En botanique, en généalogie, en informatique, en mathétiques etc.

*Similarity Join, Harmonization, Clustering, Fuzzy Clustering, Fuzzy Matching, Deduplication, Record Linkage, Entity Resolution, Consolidation, Canonicalization etc.*

---

## Les fondements algorithmiques

### Comment arrive-t-on à formuler des programmes capables de détecter des doublons potentiels ?

---

## I.
## La normalisation

### Réduire des chaînes de caractère à une clé unique pour permettre à l'ordinateur de les associer

---

#### Des caractères et du sens

Tous les caractères d'une chaîne le même impact sur son "sens". Parfois certaines choses sont optionelles pour la compréhension:

* Les espaces blancs
* La ponctuation
* La casse (majuscule/minuscules)
* Les accents (merci aux anglo-saxons...)

<p>
  <Verbatim>Université&nbsp;&nbsp;&nbsp;&nbsp;de la SORBONNE.</Verbatim>
  &nbsp;=~&nbsp;
  <Verbatim>universite de la sorbonne</Verbatim>
</p>

---

#### "Les Experts: chaîne de caractère" - Le fingerprinting

Il est possible de pousser la logique de normalisation très loin:

* L'ordre des mots
* La répétition des mots

<p>
  <Verbatim>University of north Carolina</Verbatim>
  &nbsp;=~&nbsp;
  <Verbatim>Carolina, North university of</Verbatim>
</p>


[Démo](https://yomguithereal.github.io/talisman/keyers#fingerprint)

---

#### Les algorithmes phonétiques

Les bibliothécaires et le [Soundex](https://en.wikipedia.org/wiki/Soundex) (1918, Russell, Odell).

Lawrence Philips et le [metaphone](https://en.wikipedia.org/wiki/Metaphone).

[Démos](https://yomguithereal.github.io/talisman/phonetics), [Phonogram](https://yomguithereal.github.io/phonogram/)

<p>
  <Verbatim>Michael Quail</Verbatim>
  &nbsp;=~&nbsp;
  <Verbatim>Mickael Quayle</Verbatim>
</p>

<Notes>
  L'ironie de leurs nom propres.
</Notes>

---

#### Les stemmers

Réduction à la "racine" grammaticale d'un mot.

Martin Porter et le [Porter Stemming Algorithm](https://tartarus.org/martin/PorterStemmer/index-old.html).

Ne pas confondre avec le processus de lemmatisation.

[Démos](https://yomguithereal.github.io/talisman/stemmers)

<p>
  <Verbatim>Building</Verbatim>
  &nbsp;=~&nbsp;
  <Verbatim>Build</Verbatim>
</p>

<Notes>
  Martin Porter n'est pas le premier, mais certainement le plus connu.
</Notes>

---

## II.
## La distance

### Quand la normalisation ne suffit pas, il convient de formuler un moyen de déterminer à quel point deux chaînes sont "proches" l'une de l'autre

---

#### La distance de Levenshtein

Distance absolue mesurant la distance entre deux chaînes exprimée en nombre d'opérations à effectuer pour transformer l'une en l'autre:

1. Substitution
2. Addition
3. Deletion
4. Damerau: transposition

<p>
  Levenshtein(
  <Verbatim>Levenshtein</Verbatim>
  ,&nbsp;
  <Verbatim>Levensthein</Verbatim>
  ) = <code>2</code>
</p>

[Démo](https://yomguithereal.github.io/talisman/metrics/distance#levenshtein)

<Notes>
  C'est une véritable distance métrique (au sens mathématique du terme).
</Notes>

----

#### La similarité de Jaccard

Intersection de l'ensemble des caractères de deux chaînes sur leur union.

Naturellement exprimée entre `0` et `1`, ce qui la rend plus facile à interpréter.

<p>
  Jaccard(
  <Verbatim>context</Verbatim>
  ,&nbsp;
  <Verbatim>contact</Verbatim>
  ) = <code>4/7</code>
</p>

[Démo](https://yomguithereal.github.io/talisman/metrics/distance#jaccard)

<Notes>
  Distance et son inverse: la similarité.
</Notes>

---

## III.
## L'appariement

### Maintenant que nous pouvons comparer des chaînes, comment constituer des paires de doublons potentiels ?

---

#### Le cas trivial - la distance identité

Grouper des chaînes par clé identique est un problème résolu en informatique.

Hashmaps, tris, arbres etc. `O(n)` ou `O(n log n)`.

---

#### Utiliser une distance utile

Considérons que 2 éléments sont similaires en vertu d'une heuristique de distance.

Si deux chaines ont une distance de Levenshtein inférieure à 2, par exemple, on jugera qu'elle sont suffisamment similaires.

---

#### La solution naïve

Il suffit de calculer la similarité de toutes les paires de notre jeu de données.

Il y a `(n * (n - 1)) / 2` paires possibles.

Le problème est donc quadratique `O(n²)`.

---

#### La solution naïve

* `100` => `4 950` paires
* `1000` => `499 500` paires
* `10 000` => `49 995 000` paires

En sus, les métriques de distance/similarité sont souvent couteuses.

Cette solution ne passe pas à l'échelle.

---

#### Optimisations

* `Transitive`: *Blocking, Sorted Neighborhood Method*
* `Spatiale`: *Indexation de l'espace métrique (Vantage Point Trees)*
* `Spécialisée`: *PPJoin, PassJoin etc.*
* `Probabiliste`: *NN Descent, Minhashing*

<Notes>
  Il existe des méthodes mais elles ont toutes des inconvénients ou des spécialisations. Je peux en parler pendant des heures.
</Notes>

---

## Aller plus loin

### Comment améliorer les outils existants et comment rendre leur pratique plus évidente et efficace ?

---

## I.
## Le Machine Learning

### A l'ère de l'apprentissage automatique, n'existe-t-il pas une méthode magique pour résoudre notre problème ?

---

<h2 style={{textAlign: 'center'}}>Non</h2>

---

#### L'échec relatif des méthodes classiques

Il existe bien des modèles supervisés ou non (e.g. Fellegi-Sunter).

Leur scores ne sont pas réjouissants dans la plupart des cas.

Ce qui est certain: il faudra probablement entraîner un modèle spécifique à votre jeu de données (surtout en sciences sociales).

Temps pour entraîner le modèle =~ Faire le nettoyage à la main.

<Notes>
  Je suis un peu catastrophiste mais à une échelle méso ça se tient.
</Notes>

---

#### Pourquoi cela marche-t-il mal ?

1. Les features que l'on peut trouver dans le jeu de données ne corrèlent quasiment jamais avec le fait d'être un doublon ou pas.
2. Le fait que plusieurs méthodes détectent la même paire n'est pas pertinent pour juger de la probabilité que cette paire soit un vrai doublon.

*La probabilité qu'une paire soit un vrai doublon n'a que peu de variables dépendantes.*

---

#### Frustration quantitative

Il est par ailleurs difficile d'estimer véritablement le nombre de doublons d'un jeu de données.

Quantifier ce nombre correspond à résoudre le problème en entier.

Il convient d'être très prudent avec les méthodes d'échantillonage classiques tant ce problème a tendance à échapper aux distributions usuelles.

---

#### Une alternative hybride: les systèmes de scoring

Par des méthodes heuristiques ou statistique, il reste possible de *scorer* les paires pour assister l'humain.

Mais il y a un risque: générer beaucoup de faux négatifs.

C'est une bonne stratégie pour éviter un maximum de faux positifs dans le cas d'une déduplication automatisée.

---

#### Le cas du projet BHHT

[A brief history of human time](https://hal.archives-ouvertes.fr/hal-01440325)

Des millions de pages wikipedia.

Des centaines de milliers de doublons potentiels.

---

#### Le paradoxe des anniversaires

[Lien](https://fr.wikipedia.org/wiki/Paradoxe_des_anniversaires)

23 personnes = 1 chance sur 2

Les gens sont en général mauvais juges des probabilités.

---

#### Le rapport avec la choucroute

1. Quelle est la probabilité, sur Wikipedia, que 2 individus aient un nom similaire, la même date de naissance et de mort et la même occupation?
2. Parfois le dé est pipé.

[Tim Coronel](https://en.wikipedia.org/wiki/Tim_Coronel) & [Tom Coronel](https://en.wikipedia.org/wiki/Tom_Coronel)

[Lauren Vélez](https://fr.wikipedia.org/wiki/Lauren_V%C3%A9lez) & [Lorraine Vélez](https://en.wikipedia.org/wiki/Lorraine_V%C3%A9lez)

---

#### La fréquence est-elle un bon garde-fou ?

Si un individu est célèbre, i.e. a une importance statistique dans mon jeu de données, alors les doublons sont moins probables?

Ou la fréquence des deux individus d'une paires conditionne la probabilité du doublon?

Non. Mais il convient probablement de vérifier ces cas-là en priorité car ils affectent bien plus vos agrégations.

---

#### Charles Dickens le botaniste

<p style={{overflowY: 'scroll', overflowX: 'scroll', height: '300px', border: '1px dashed black', padding: '20px', fontFamily: 'monospace', whiteSpace: 'pre'}}>
  {DATA.bhht.map(item => <span key={item}>{item}<br /></span>)}
</p>

---

#### Et si vos features sont erronées :) ?

---

## II.
## Vers une meilleure UX

### Les outils doivent s'améliorer et accompagner l'utilisateur dans la compréhension des heuristiques mobilisées

---

#### Retour sur TOFLIT18

1. Manque de maîtrise à l'époque
2. Besoin de compétences métiers

<ul style={{overflowY: 'scroll', height: '250px', border: '1px dashed black'}}>
  {DATA.mercure.map(item => <li key={item}><code>{item}</code></li>)}
</ul>

<Notes>
  Pourquoi ne pas avoir utilisé tous ces algorithmes?
</Notes>

---

#### Un peu de recul

Le travail de déduplication assistée par ordinateur est typique du **quali-quanti**.

Surtout dans les échelles méso sur lesquelles les sciences sociales ont tendance à se positionner.

Ce genre de travail a besoin de nuance et d'artisanat.

---

#### Le besoin de méthodologies spécialisées

1. Il n'existe pas de balle d'argent
2. Il existe beaucoup de méthodes, de distances, de traitements, de paramètres: il faut pouvoir choisir les bons voire les créer soi-même
3. La plupart des méthodes existantes ciblent l'anglais et la culture anglo-saxonne.

*Il faut introduire de la pédagogie dans les outils*

<Notes>
  Parler de Carry.
</Notes>

---

#### La sulfateuse d'Open•Refine

Cribler les méthodes en allant de la moins à la plus *agressive* reste une très bonne approche. Il faut juste mettre ces méthodes au goût du jour.

Les outils existants négligent de sauvegarder les faux positifs.

---

#### Il y a le bon et le mauvais cluster

Problème majeur: considérer les paires ou les clusters?

Les paires empêchent de voir le *big picture*, mais les clusters, surtout dans le cas des fuzzy clusters, peuvent rendre le processus de criblage confus.

Et si l'on choisit de considérer de cluster, comment le constituer?

---

#### Le bon cluster il prend son fusil

Contradiction:

1. Un bon cluster devrait avoir un rayon cohérent avec le sens de distance choisi.
2. La relation de similarité est globalement transitive.

---

#### La méthode du "leader"

```
DATA = ('abc', 'bcd', 'cde', 'def', 'efg', 'ghi')

CLUSTERS = ('abc', 'bcd'), ('cde', 'def'), ('efg', 'ghi')
```

* Le rayon est correct
* L'ordre joue sur les clusters
* Pas de fuzzy clusters

---

#### La méthode des fuzzy clusters

```
DATA = ('abc', 'bcd', 'cde', 'def', 'efg', 'ghi')

CLUSTERS = ('abc', 'bcd'), ('cde', 'bcd', 'def')
           ('efg', 'def', 'fgh'), ('ghi', 'fgh')
```

* Le rayon est correct
* L'ordre joue sur les clusters
* Fuzzy clusters

<Notes>C'est la méthode d'Open Refine</Notes>

---

#### La méthode des composantes connexes

```
DATA = ('abc', 'bcd', 'cde', 'def', 'efg', 'ghi')

CLUSTERS =
  ('abc', 'bcd', 'cde', 'def', 'efg', 'ghi')
```

* Le rayon n'est pas correct
* L'ordre ne joue pas sur les clusters
* Pas de fuzzy clusters

---

#### Ma préférence, à moi

Les composantes connexes.

1. Dans notre espace très épars, la probabilité d'avoir une chaîne est très faible.
2. Les fuzzy clusters sont une plaie pour l'UX.
3. La composition des clusters est déterministes.

---

#### Vers des interfaces exploratoires ?

Explorer l'espace métrique n'est pas facile.

D'autant plus quand il est épars.

[Démo](http://toflit18.medialab.sciences-po.fr/#/exploration/terms) sur TOFLIT18.

---

#### Vers des interfaces préventives ?

Le futur du projet [Ricardo](http://toflit18.medialab.sciences-po.fr/#/exploration/terms).

Assurer la qualité des données au fil du temps et de l'ajout de nouvelles sources.

Donner les moyens d'éviter de nouvelles incohérences.

---

#### Vers des interfaces de jointure ?

Il y a des interfaces pour le self similarity join. Mais beaucoup moins pour la similarity join.

<Notes>Cas des projets mapping french russia etc.</Notes>

---

#### Vers de meilleures heuristiques de normalisation ?

Pour que cette pratique puisse passer à l'échelle il convient les méthodes s'affinent pour générer moins de faux positifs tout en limitant le nombre de faux négatifs.

Pour cela il faut appliquer des compétences métiers.

Le projet [Phonogram](https://yomguithereal.github.io/phonogram/) et [Rusalka](https://github.com/Yomguithereal/fog/blob/master/fog/phonetics/rusalka.py).

<Notes>Le voyage au Pérou.</Notes>

---

<h2 style={{textAlign: 'center'}}>Merci pour votre attention!</h2>

---

#### Quelques liens

* OpenRefine: [http://openrefine.org/](http://openrefine.org/)
* Takoyaki: [https://yomguithereal.github.io/takoyaki/](https://yomguithereal.github.io/takoyaki/)
* Talisman: [https://yomguithereal.github.io/talisman/](https://yomguithereal.github.io/talisman/)
* Fog: [https://github.com/Yomguithereal/fog/](https://github.com/Yomguithereal/fog/)
