---
slug: lru-cache
title: Implementing an efficient LRU cache for JavaScript
subtitle:
  Where we discover how to harness the power of JavaScript's typed array to go
  back to the memory allocation schemes of older & more static languages
---

import {Divider, MarginNote, SideNote} from '../components/tufte';
import SafeLink from '../components/SafeLink';

Let's say we need to process a very large, hundreds of gigabytes large, csv file containing urls we will need to fetch. To ensure we are not going to run out of memory while parsing the whole file, we need to read the file line by line:

<!-- TODO: make a jump to the juicy part -->

```js
csv.forEachLine(line => {
  fetch(line.url);
});
```

<p>
  Now let's say the person that created this file forgot to deduplicate the urls.
  <SideNote id="urls-dedup">I am aware that we could easily solve the issue by using <code>sort -u</code> but for the purpose of the demonstration, let's imagine this kind of solution does not exist.</SideNote>
  <SideNote id="ural">Deduping urls is not as straigthforward as it may seem. Check the <SafeLink href="https://github.com/medialab/ural#readme">ural</SafeLink> library in python, for instance, for some examples of what can be achieved in this regard.</SideNote>
  This is an issue because we don't want to fetch the same url more than once: grabbing resources from the web is time-consuming & should be done parcimoniously not to overflow the sites you are grabbing those resources from.
</p>

An easy solution could be to remember the url we already fetched by storing them into a map. Our pseudo-code would be changed to the following:

```js
const done = new Map();

csv.forEachLine(line => {
  if (done.has(line.url))
    return done.get(line.url);

  const result = fetch(line.url);
  done.set(line.url, result);
});
```

At this point, the astute reader has easily noticed that we just defeated the purpose of reading the csv file line by line since we now need to commit all its urls to memory.

And herein lies the issue: we need a way not to fetch the same urls too much while also making sure we won't run out of memory.

<p>
  <MarginNote><em>Fun fact</em>: if you work with data coming from the Internet such as lists of crawled urls, you will inevitably stumble upon this kind of power law. It naturally occurs because people link exponentially more to <code>twitter.com</code> than <code>unknownwebsite.fr</code>.</MarginNote>
  Fortunately for us, it seems that only a tiny fraction of the urls contained in our file are repeated very often while the vast majority of others only appears one or two times. We can leverage this fact by designing a policy to throw away urls from our map if we have a good intuition they are unlikely to appear again. Thus, we won't allow our map to exceed a predetermined amount of memory.
</p>

As such, one of the most commonly used eviction policy for such cases is called **LRU**, for **L**east **R**ecently **U**sed.

# The LRU cache

Hence, we understand that a LRU cache is a fixed-capacity map able to bind values to keys with the following twist: if the cache is full but we need to insert a new key-value pair, it will make some place by evicting the least recently used key-value pair.

To do so, the cache will need to store given pairs in order of their last access. This means that each time someone tries to set a new key, or access a key, we need to modify the underlying list of pairs to ensure the needed order is maintained.

<p>
  <MarginNote>Note that LFU (Least Frequently Used) is a perfectly valid cache eviction policy. It's just less widespread. You can read about it <SafeLink href="https://en.wikipedia.org/wiki/Cache_replacement_policies#Least-frequently_used_(LFU)">here</SafeLink> and find implementation notes <SafeLink href="https://www.geeksforgeeks.org/lfu-least-frequently-used-cache-implementation/">here</SafeLink></MarginNote>
  But why is this order relevant? Why not record the number of times each pair is accessed instead and evict the least frequently used item? Here are some reasons why:
</p>

* LRU is a actually a good proxy of LFU since the more a pair is accessed, the less chance it has to be evicted.
* You will need to store integers in addition to everything else to keep track of the number of times the pairs were accessed.
* Infering the order of pair access is very straightforward since it can be deduced from the order of operations on the cache.
* With LFU, you will often need to make an arbitrary choice of which pair to evict, for instance if all your pairs have been accessed only once. With LRU, you don't have such choice to make: you just evict the least recently used, which cannot be ambiguous.
* LRU is quite straightforward to implement, LFU is not.

# Implementing a LRU cache

There are many ways to implement a working LRU cache but I will only focus here on the way you are most likely to encounter in the wild when developing with high-level languages.

Usually, to implement a proper LRU cache, one will need the two following ingredients:

1. A hashmap-like data structure able to retrieve values associated to arbitrary keys - such as strings - efficiently. In JavaScript we can use either the ES6 `Map` or any plain object `{}`: remember that our cache is no different from a fixed-capacity key-value store.
2. A way to store our pairs in the order of their last access. What's more, we will need to be able to travers the list in both normal and reversed order. That's why people naturally lean toward a [doubly-linked list](https://en.wikipedia.org/wiki/Doubly_linked_list) to do the job.

Then, your implementation needs to be able to run the two following operations:

* `#.set`: associating a value to the given key, while evicting the least recently used item if the cache is full.
* `#.get`: retrieving the value associated to the given key if this one exists at all in the cache.

And here is how we could use such a cache:

```js
// Let's create a cache able to contain 3 items
const cache = new LRUCache(3);

// Let's add items
cache.set(1, 'one');
cache.set(2, 'two');
cache.set(3, 'three');

// Up until now, nothing was evicted from the cache
cache.get(2);
>>> 'two'

// Oh no! we need to add a new pair
cache.set(4, 'four');

// `1` was evicted because it was the least recently used key
cache.get(1);
>>> undefined

// If you get `2`, it won't be the lru anymore
cache.get(2);
>>> 'two'

// Which means that it's `3` that will be evicted now!
cache.set(5, 'five');
cache.get(3);
>>> undefined

// Thus we never store more than 3 pairs
cache.size
>>> 3
```
