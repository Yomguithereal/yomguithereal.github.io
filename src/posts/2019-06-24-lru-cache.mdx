---
slug: lru-cache
title: Implementing an efficient LRU cache for JavaScript
subtitle:
  Where we discover how to harness the power of JavaScript's typed array to go
  back to the memory allocation schemes of older & more static languages
---

import {Divider, MarginNote, SideNote} from '../components/tufte';
import SafeLink from '../components/SafeLink';

Let's say we need to process a very large, hundreds of gigabytes large, csv file containing urls we will need to fetch. To ensure we are not going to run out of memory while parsing the whole file, we need to read the file line by line:

<!-- TODO: make a jump to the juicy part -->
<!-- TODO: link to mnemonist implem -->
<!-- TODO: what about asm.js and wasm? -->
<!-- TODO: splay tree -->
<!-- TODO: hashmaps -->
<!-- TODO: trick to save up pointers -->
<!-- TODO: rolling your own system, addresses in memory: indices in typed array -->

```js
csv.forEachLine(line => {
  fetch(line.url);
});
```

<p>
  Now let's say the person that created this file forgot to deduplicate the urls.
  <SideNote id="urls-dedup">I am aware that we could easily solve the issue by using <code>sort -u</code> but for the purpose of the demonstration, let's imagine this kind of solution does not exist.</SideNote>
  <SideNote id="ural">Deduping urls is not as straigthforward as it may seem. Check the <SafeLink href="https://github.com/medialab/ural#readme">ural</SafeLink> library in python, for instance, for some examples of what can be achieved in this regard.</SideNote>
  This is an issue because we don't want to fetch the same url more than once: grabbing resources from the web is time-consuming & should be done parcimoniously not to overflow the sites you are grabbing those resources from.
</p>

An easy solution could be to remember the url we already fetched by storing them into a map. Our pseudo-code would be changed to the following:

```js
const done = new Map();

csv.forEachLine(line => {
  if (done.has(line.url))
    return done.get(line.url);

  const result = fetch(line.url);
  done.set(line.url, result);
});
```

At this point, the astute reader has easily noticed that we just defeated the purpose of reading the csv file line by line since we now need to commit all its urls to memory.

And herein lies the issue: we need a way not to fetch the same urls too much while also making sure we won't run out of memory.

<p>
  <MarginNote><em>Fun fact</em>: if you work with data coming from the Internet such as lists of crawled urls, you will inevitably stumble upon this kind of power law. It naturally occurs because people link exponentially more to <code>twitter.com</code> than <code>unknownwebsite.fr</code>.</MarginNote>
  Fortunately for us, it seems that only a tiny fraction of the urls contained in our file are repeated very often while the vast majority of others only appears one or two times. We can leverage this fact by designing a policy to throw away urls from our map if we have a good intuition they are unlikely to appear again. Thus, we won't allow our map to exceed a predetermined amount of memory.
</p>

As such, one of the most commonly used eviction policy for such cases is called **LRU**, for **L**east **R**ecently **U**sed.

# The LRU cache

Hence, we understand that a LRU cache is a fixed-capacity map able to bind values to keys with the following twist: if the cache is full but we need to insert a new key-value pair, it will make some place by evicting the least recently used key-value pair.

To do so, the cache will need to store given pairs in order of their last access. This means that each time someone tries to set a new key, or access a key, we need to modify the underlying list of pairs to ensure the needed order is maintained.

<p>
  <MarginNote><em>Note</em>: LFU (Least Frequently Used) is a perfectly valid cache eviction policy. It's just less widespread. You can read about it <SafeLink href="https://en.wikipedia.org/wiki/Cache_replacement_policies#Least-frequently_used_(LFU)">here</SafeLink> and find implementation notes <SafeLink href="https://www.geeksforgeeks.org/lfu-least-frequently-used-cache-implementation/">here</SafeLink></MarginNote>
  But why is this order relevant? Why not record the number of times each pair is accessed instead and evict the least frequently used item? Here are some reasons why:
</p>

* LRU is a actually a good proxy of LFU since the more a pair is accessed, the less chance it has to be evicted.
* You will need to store integers in addition to everything else to keep track of the number of times the pairs were accessed.
* Infering the order of pair access is very straightforward since it can be deduced from the order of operations on the cache.
* With LFU, you will often need to make an arbitrary choice of which pair to evict, for instance if all your pairs have been accessed only once. With LRU, you don't have such choice to make: you just evict the least recently used, which cannot be ambiguous.
* LRU is quite straightforward to implement, LFU is not.

# Implementing a LRU cache

There are many ways to implement a working LRU cache but I will only focus here on the way you are most likely to encounter in the wild when developing with high-level languages.

Usually, to implement a proper LRU cache, one will need the two following ingredients:

1. A hashmap-like data structure able to retrieve values associated to arbitrary keys - such as strings - efficiently. In JavaScript we can use either the ES6 `Map` or any plain object `{}`: remember that our cache is no different from a fixed-capacity key-value store.
2. A way to store our pairs in the order of their last access. What's more, we will need to be able to travers the list in both normal and reversed order. That's why people naturally lean toward a [doubly-linked list](https://en.wikipedia.org/wiki/Doubly_linked_list) to do the job.

Minimally, your implementation needs to be able to run the two following operations:

* `#.set`: associating a value to the given key, while evicting the least recently used item if the cache is full.
* `#.get`: retrieving the value associated to the given key if this one exists at all in the cache.

And here is how we could use such a cache:

```js
// Let's create a cache able to contain 3 items
const cache = new LRUCache(3);

// Let's add items
cache.set(1, 'one');
cache.set(2, 'two');
cache.set(3, 'three');

// Up until now, nothing was evicted from the cache
cache.get(2);
>>> 'two'

// Oh no! we need to add a new pair
cache.set(4, 'four');

// `1` was evicted because it was the least recently used key
cache.get(1);
>>> undefined

// If you get `2`, it won't be the lru anymore
cache.get(2);
>>> 'two'

// Which means that it's `3` that will be evicted now!
cache.set(5, 'five');
cache.get(3);
>>> undefined

// Thus we never store more than 3 pairs
cache.size
>>> 3
```

# Doubly-linked lists

<p>
  <MarginNote>
    <em>Question</em>: Why isn't a singly-linked list enough in our case? Because we have to efficiently perform the following operations on our list:
    <ol>
      <li>place an item at the beginning of the list</li>
      <li>move an item from anywhere in the list to its beginning</li>
      <li>remove the last item from the list while keeping a correct pointer to the new last item</li>
    </ol>
  </MarginNote>
  To be able to implement our LRU cache, we will first need to implement a doubly-linked list to make sure we are able to store our items in order of their last access: the least recently used item will be at the end of this list while the most recently used will be at its beginning.
</p>

So how do we represent a doubly-linked list in memory? Usually, we do so by creating a structure containing the following three elements:

1. A payload, i.e. the actual list item. It can be anything, really, from an string to an integer etc.
2. A pointer towards the previous element in the list.
3. A pointer towards the next element in the list.

Then we also need to store a pointer to both the first & the last element of the list and we are done.

Schematically, it would look somewhat like this:

```c
a node:

  (prev|payload|next)

  payload: the stored value. here, it's a string
  prev: pointer to previous item
  next: pointer to next item
  •: pointer
  x: null pointer

       ┌─────>┐   ┌───────>┐   ┌────────>┐
  head • (x|"one"|•)  (•|"two"|•)  (•|"three"|x) • tail
              └<───────┘   └<───────┘    └<──────┘
```

# LRU Cache-related doubly linked lists operations

As long as the cache is not full, it is quite easy to maintain our list of cached items. We just need to prepend the newly inserted items into the list:

```c
1. an empty cache with capacity 3


  head x     x tail


2. let's cache the key "one"

       ┌─────>┐
  head • (x|"one"|x) • tail
              └<─────┘

3. let's cache the key "two" (notice "two" comes at the front)

       ┌─────>┐   ┌───────>┐
  head • (x|"two"|•)  (•|"one"|x) • tail
              └<───────┘   └<─────┘

4. finally we cache the key "three"

       ┌──────>┐    ┌───────>┐   ┌───────>┐
  head • (x|"three"|•)  (•|"two"|•)  (•|"one"|x) • tail
               └<────────┘   └<───────┘   └<─────┘
```

So far so good. Now, to keep our list in LRU order, if anyone accesses an already stored key in the cache we will need to reorder the list by moving said key to the front:

```c
1. the current state of our cache

       ┌──────>┐    ┌───────>┐   ┌───────>┐
  head • (x|"three"|•)  (•|"two"|•)  (•|"one"|x) • tail
               └<────────┘   └<───────┘   └<─────┘

2. we access the key "two", we first extract it from the list and
   rewire previous & next items.

       ┌──────>┐    ┌───────>┐
  head • (x|"three"|•)  (•|"one"|x) • tail
               └<────────┘   └<─────┘

  dangling: (x|"two"|x)

3. then we move the node to the front

       ┌─────>┐   ┌────────>┐    ┌───────>┐
  head • (x|"two"|•)  (•|"three"|•)  (•|"one"|x) • tail
               └<──────┘    └<────────┘   └<─────┘
```

Note that each time, we need to update the head pointer and that, sometimes, we also need to update the tail pointer.

Finally, if the cache is already full and we need to insert a yet unknown key, we will need to pop the last item from the list to make place so we can prepend the new one.

```c
1. the current state of our cache

       ┌─────>┐   ┌────────>┐    ┌───────>┐
  head • (x|"two"|•)  (•|"three"|•)  (•|"one"|x) • tail
               └<──────┘    └<────────┘   └<─────┘

2. we need to insert the key "six" but the cache is full
   we first need to pop "one", being the lru item

       ┌─────>┐   ┌────────>┐
  head • (x|"two"|•)  (•|"three"|•) • tail
               └<──────┘    └<──────┘

3. then we prepend our new item
       ┌─────>┐   ┌───────>┐   ┌────────>┐
  head • (x|"six"|•)  (•|"two"|•)  (•|"three"|x) • tail
              └<───────┘   └<───────┘    └<──────┘
```

Here is all you need to know about doubly-linked lists to be able to implement a decent LRU cache.

# Implementing doubly-linked lists in JavaScript

Now we have a slight issue: the JavaScript language does not have pointers per se. Indeed we can only work by passing references around, represented as object properties.
