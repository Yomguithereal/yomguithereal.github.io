---
slug: lru-cache
title: Implementing an efficient LRU cache for JavaScript
subtitle:
  Where we discover how to harness the power of JavaScript's typed array to go
  back to the memory allocation schemes of older & more static languages
---

import {MarginNote, SideNote} from '../components/tufte';
import SafeLink from '../components/SafeLink';

<small style={{textAlign: 'center'}}><a href="#implementing-doubly-linked-lists-in-javascript">I already know about LRU caches and doubly-linked lists. Bring me to the juicy part dammit!</a></small>

Let's say we need to process a very large, hundreds of gigabytes large, csv file containing urls we will need to fetch. To ensure we are not going to run out of memory while parsing the whole file, we need to read the file line by line:

<!-- TODO: link to mnemonist implem -->
<!-- TODO: fastlink to mnemonist? -->
<!-- TODO: what about asm.js and wasm? -->
<!-- TODO: splay tree -->
<!-- TODO: hashmaps -->
<!-- TODO: trick to save up pointers -->
<!-- TODO: rolling your own system, addresses in memory: indices in typed array -->
<!-- TODO: link to the bench, advantages: no memory allocation, saves up memory, fast array indices lookup -->
<!-- TODO: quand le moteur n'a rien a deviner tout est plus simple/rapide | no GC apart from key set/remove in the map -->
<!-- Dans la réalité du monde véritable -->

```js
csv.forEachLine(line => {
  fetch(line.url);
});
```

<p>
  Now let's say the person that created this file forgot to deduplicate the urls.
  <SideNote id="urls-dedup">I am aware that we could easily solve the issue by using <code>sort -u</code> but for the purpose of the demonstration, let's imagine this kind of solution does not exist.</SideNote>
  <SideNote id="ural">Deduping urls is not as straigthforward as it may seem. Check the <SafeLink href="https://github.com/medialab/ural#readme">ural</SafeLink> library in python, for instance, for some examples of what can be achieved in this regard.</SideNote>
  This is an issue because we don't want to fetch the same url more than once: grabbing resources from the web is time-consuming & should be done parcimoniously not to overflow the sites you are grabbing those resources from.
</p>

An easy solution could be to remember the url we already fetched by storing them into a map. Our pseudo-code would be changed to the following:

```js
const done = new Map();

csv.forEachLine(line => {
  if (done.has(line.url))
    return done.get(line.url);

  const result = fetch(line.url);
  done.set(line.url, result);
});
```

At this point, the astute reader has easily noticed that we just defeated the purpose of reading the csv file line by line since we now need to commit all its urls to memory.

And herein lies the issue: we need a way not to fetch the same urls too much while also making sure we won't run out of memory.

<p>
  <MarginNote><em>Fun fact</em>: if you work with data coming from the Internet such as lists of crawled urls, you will inevitably stumble upon this kind of power law. It naturally occurs because people link exponentially more to <code>twitter.com</code> than <code>unknownwebsite.fr</code>.</MarginNote>
  Fortunately for us, it seems that only a tiny fraction of the urls contained in our file are repeated very often while the vast majority of others only appears one or two times. We can leverage this fact by designing a policy to throw away urls from our map if we have a good intuition they are unlikely to appear again. Thus, we won't allow our map to exceed a predetermined amount of memory.
</p>

As such, one of the most commonly used eviction policy for such cases is called **LRU**, for **L**east **R**ecently **U**sed.

# The LRU cache

Hence, we understand that a LRU cache is a fixed-capacity map able to bind values to keys with the following twist: if the cache is full but we need to insert a new key-value pair, it will make some place by evicting the least recently used key-value pair.

To do so, the cache will need to store given pairs in order of their last access. This means that each time someone tries to set a new key, or access a key, we need to modify the underlying list of pairs to ensure the needed order is maintained.

<p>
  <MarginNote><em>Note</em>: LFU (Least Frequently Used) is a perfectly valid cache eviction policy. It's just less widespread. You can read about it <SafeLink href="https://en.wikipedia.org/wiki/Cache_replacement_policies#Least-frequently_used_(LFU)">here</SafeLink> and find implementation notes <SafeLink href="https://www.geeksforgeeks.org/lfu-least-frequently-used-cache-implementation/">here</SafeLink></MarginNote>
  But why is this order relevant? Why not record the number of times each pair is accessed instead and evict the least frequently used item? Here are some reasons why:
</p>

* LRU is a actually a good proxy of LFU since the more a pair is accessed, the less chance it has to be evicted.
* You will need to store integers in addition to everything else to keep track of the number of times the pairs were accessed.
* Infering the order of pair access is very straightforward since it can be deduced from the order of operations on the cache.
* With LFU, you will often need to make an arbitrary choice of which pair to evict, for instance if all your pairs have been accessed only once. With LRU, you don't have such choice to make: you just evict the least recently used, which cannot be ambiguous.
* LRU is quite straightforward to implement, LFU is not.

# Implementing a LRU cache

There are many ways to implement a working LRU cache but I will only focus here on the way you are most likely to encounter in the wild when developing with high-level languages.

Usually, to implement a proper LRU cache, one will need the two following ingredients:

1. A hashmap-like data structure able to retrieve values associated to arbitrary keys - such as strings - efficiently. In JavaScript we can use either the ES6 `Map` or any plain object `{}`: remember that our cache is no different from a fixed-capacity key-value store.
2. A way to store our pairs in the order of their last access. What's more, we will need to be able to travers the list in both normal and reversed order. That's why people naturally lean toward a [doubly-linked list](https://en.wikipedia.org/wiki/Doubly_linked_list) to do the job.

Minimally, your implementation needs to be able to run the two following operations:

* `#.set`: associating a value to the given key, while evicting the least recently used item if the cache is full.
* `#.get`: retrieving the value associated to the given key if this one exists at all in the cache.

And here is how we could use such a cache:

```js
// Let's create a cache able to contain 3 items
const cache = new LRUCache(3);

// Let's add items
cache.set(1, 'one');
cache.set(2, 'two');
cache.set(3, 'three');

// Up until now, nothing was evicted from the cache
cache.get(2);
>>> 'two'

// Oh no! we need to add a new pair
cache.set(4, 'four');

// `1` was evicted because it was the least recently used key
cache.get(1);
>>> undefined

// If you get `2`, it won't be the lru anymore
cache.get(2);
>>> 'two'

// Which means that it's `3` that will be evicted now!
cache.set(5, 'five');
cache.get(3);
>>> undefined

// Thus we never store more than 3 pairs
cache.size
>>> 3
```

# Doubly-linked lists

<p>
  <MarginNote>
    <em>Question</em>: Why isn't a singly-linked list enough in our case? Because we have to efficiently perform the following operations on our list:
    <ol>
      <li>place an item at the beginning of the list</li>
      <li>move an item from anywhere in the list to its beginning</li>
      <li>remove the last item from the list while keeping a correct pointer to the new last item</li>
    </ol>
  </MarginNote>
  To be able to implement our LRU cache, we will first need to implement a doubly-linked list to make sure we are able to store our items in order of their last access: the least recently used item will be at the end of this list while the most recently used will be at its beginning.
</p>

So how do we represent a doubly-linked list in memory? Usually, we do so by creating a structure containing the following three elements:

1. A payload, i.e. the actual list item. It can be anything, really, from an string to an integer etc.
2. A pointer towards the previous element in the list.
3. A pointer towards the next element in the list.

Then we also need to store a pointer to both the first & the last element of the list and we are done.

Schematically, it would look somewhat like this:

```c
a node:

  (prev|payload|next)

  payload: the stored value. here, it's a string
  prev: pointer to previous item
  next: pointer to next item
  •: pointer
  x: null pointer

a list:

       ┌─────>┐   ┌───────>┐   ┌────────>┐
  head • (x|"one"|•)  (•|"two"|•)  (•|"three"|x) • tail
              └<───────┘   └<───────┘    └<──────┘
```

# LRU Cache-related doubly linked lists operations

As long as the cache is not full, it is quite easy to maintain our list of cached items. We just need to prepend the newly inserted items into the list:

```c
1. an empty cache with capacity 3


  head x     x tail


2. let's cache the key "one"

       ┌─────>┐
  head • (x|"one"|x) • tail
              └<─────┘

3. let's cache the key "two" (notice "two" comes at the front)

       ┌─────>┐   ┌───────>┐
  head • (x|"two"|•)  (•|"one"|x) • tail
              └<───────┘   └<─────┘

4. finally we cache the key "three"

       ┌──────>┐    ┌───────>┐   ┌───────>┐
  head • (x|"three"|•)  (•|"two"|•)  (•|"one"|x) • tail
               └<────────┘   └<───────┘   └<─────┘
```

So far so good. Now, to keep our list in LRU order, if anyone accesses an already stored key in the cache we will need to reorder the list by moving said key to the front:

```c
1. the current state of our cache

       ┌──────>┐    ┌───────>┐   ┌───────>┐
  head • (x|"three"|•)  (•|"two"|•)  (•|"one"|x) • tail
               └<────────┘   └<───────┘   └<─────┘

2. we access the key "two", we first extract it from the list and
   rewire previous & next items.

       ┌──────>┐    ┌───────>┐
  head • (x|"three"|•)  (•|"one"|x) • tail
               └<────────┘   └<─────┘

  dangling: (x|"two"|x)

3. then we move the node to the front

       ┌─────>┐   ┌────────>┐    ┌───────>┐
  head • (x|"two"|•)  (•|"three"|•)  (•|"one"|x) • tail
              └<───────┘    └<────────┘   └<─────┘
```

Note that each time, we need to update the head pointer and that, sometimes, we also need to update the tail pointer.

Finally, if the cache is already full and we need to insert a yet unknown key, we will need to pop the last item from the list to make place so we can prepend the new one.

```c
1. the current state of our cache

       ┌─────>┐   ┌────────>┐    ┌───────>┐
  head • (x|"two"|•)  (•|"three"|•)  (•|"one"|x) • tail
              └<───────┘    └<────────┘   └<─────┘

2. we need to insert the key "six" but the cache is full
   we first need to pop "one", being the lru item

       ┌─────>┐   ┌────────>┐
  head • (x|"two"|•)  (•|"three"|•) • tail
              └<───────┘    └<──────┘

3. then we prepend our new item
       ┌─────>┐   ┌───────>┐   ┌────────>┐
  head • (x|"six"|•)  (•|"two"|•)  (•|"three"|x) • tail
              └<───────┘   └<───────┘    └<──────┘
```

Here is all you need to know about doubly-linked lists to be able to implement a decent LRU cache.

# Implementing doubly-linked lists in JavaScript

Now we have a slight issue: the JavaScript language does not have pointers per se. Indeed we can only work by passing references around, represented as object properties.

This means that, usually, most people will implement linked lists in JavaScript by relying on classes that would look like the following:

```js
// One class for the nodes
function DoublyLinkedListNode(value) {
  this.value = value;
  this.previous = null;
  this.next = null;
}

// One class for the list
function DoublyLinkedList() {
  this.head = null;
  this.tail = null;
}

// Performing operations (should be wrapped in the list's method)
const list = new DoublyLinkedList();
const node1 = new DoublyLinkedList('one');
const node2 = new DoublyLinkedList('two');

list.head = node1;
list.tail = node2;
node1.next = node2;
node2.previous = node1;
// ...
```

<p>
  While there is nothing wrong with this approach, it still has its drawback that will make any similar-looking implementation quite bad, performance-wise:
  <SideNote id="linked-list-bad-perf">This is why you won't see many people using linked lists in applicative JavaScript code.<br /><br />Only people needing it for very specific use cases where they algorithmically shine will actually use them.<br /><br />node.js has such an implementation <SafeLink href="https://github.com/nodejs/node/blob/master/lib/internal/linkedlist.js">here</SafeLink> and you can find it used for timers <SafeLink href="https://github.com/nodejs/node/blob/master/lib/internal/timers.js">here</SafeLink></SideNote>
</p>

1. Each time you instantiate a node, some superflous memory will be allocated for the object's bookkeeping.
2. If your list moves fast, i.e. nodes are often added or removed, it will trigger garbage collection, whose behavior is, in JavaScript, beyond your control.
3. Engines will try to optimize your objects as low-level structs most of the time, as opposed to a hashmap, but you have no control over it.
4. Finally, and this is related to `3.`, object property access is not the fastest thing in the world.

But here we can be a little more clever than that. Indeed, there is a characteristic of our linked list that we can leverage to be more performant: its capacity is fixed.

So, instead of using JavaScript references & properties as pointers, let's roll our own pointer system using [Typed Arrays](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray)!

# A custom pointer system

Typed Arrays are very handy JavaScript objects able to represent fixed-capacity arrays containing a certain amount of typical number types such as `int32` or `float64` and so on...

They are fairly fast and consume very little memory because the stored items are statically typed and therefore induce no bookkeeping overhead.

Here is how you would use one:

```js
// Let's create a typed array containing 256 unsigned 16 bits integers
const array = new Uint16Array(256);

// Every item is initially set to 0.
// You can read/set items the same way you would with a vanilla Array
array[10] = 34;
array[10];
>>> 34

// But here's the catch: you cannot push new items
array.push(45);
>>> throw `TypeError: array.push is not a function`
```

<p>
  <MarginNote>What's more, instantiating a typed array in JavaScript is not so far, conceptually, from calling a <code>malloc</code> in C. Or, at least, you can somewhat use them to perform the same kind of tasks in both languages.</MarginNote>
  Since we can now use those very performant arrays, why shouldn't we use them to implement our own pointer system? After all, pointers are nothing more than addresses mapping a chunk of memory.
</p>

<p>
  Then let's use a typed array as our chunk of memory and let's use indices into it as our addresses! The only tricky part is to correctly choose the integer type, relative to our capacity, to avoid overflows.
  <SideNote id="get-pointer-array">If you are too lazy or are unsure how to develop this yourself, check this function right <SafeLink href="https://github.com/Yomguithereal/mnemonist/blob/7ea90e6fec46b4c2283ae88f173bfb19ead68734/utils/typed-arrays.js#L8-L54">here</SafeLink></SideNote>
</p>

So, in order to implement a fixed-capacity doubly-linked list in JavaScript to serve our LRU cache structure, we'll need the following typed arrays:

1. A vanilla array to store our list's payloads, i.e. key-value pairs. Or two typed or vanilla arrays to store keys & values separately and depending on their respective types. For instance, if we have the guarantee our value cannot be anything else than 32 bits integers, we can again leverage typed arrays for that.
2. A typed array storing a bunch of indices representing our next pointers.
3. Another one to store the indices representing our previous pointers.

<!-- TODO: usually have to keep the 0 as null value beware! -->
